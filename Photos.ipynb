{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "naval-gravity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import regex as re\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "superior-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reviews = pd.read_csv(\"English_Reviews.csv\")\n",
    "data = pd.read_csv(\"./Dataset/Filtered_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "younger-conservative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bed_bath_table           11087\n",
       "health_beauty             9641\n",
       "sports_leisure            8624\n",
       "furniture_decor           8315\n",
       "computers_accessories     7801\n",
       "housewares                6945\n",
       "watches_gifts             5968\n",
       "telephony                 4517\n",
       "garden_tools              4336\n",
       "auto                      4226\n",
       "Name: product_category_name_english, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['product_category_name_english'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exciting-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reviews = data_reviews[data_reviews['english_comment'] != 'emp']\n",
    "data_reviews = data_reviews.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "greek-fundamentals",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41739"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_reviews.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "numerous-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in data_reviews.iterrows():\n",
    "    val = data_reviews['english_comment'].values[i].replace('�','')\n",
    "    val = val.lower()\n",
    "    val = val.split()\n",
    "    tab = str.maketrans('','',string.digits)\n",
    "    val = [w.translate(tab) for w in val]\n",
    "    tab = str.maketrans('','',string.punctuation)\n",
    "    val = [w.translate(tab) for w in val]\n",
    "    words = [w for w in val if not w in stop_words]\n",
    "    val = ' '.join([token.lemma_ for wd in words for token in nlp(wd)])\n",
    "    data_reviews.at[i,'clean_comment'] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "accredited-hawaii",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_comment_title</th>\n",
       "      <th>review_comment_message</th>\n",
       "      <th>review_creation_date</th>\n",
       "      <th>review_answer_timestamp</th>\n",
       "      <th>english_comment</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound</th>\n",
       "      <th>comp_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e64fb393e7b32834bb789ff8bb30750e</td>\n",
       "      <td>658677c97b385a9be170737859d3511b</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recebi bem antes do prazo estipulado.</td>\n",
       "      <td>21/04/2017 00:00</td>\n",
       "      <td>21/04/2017 22:02</td>\n",
       "      <td>I received well before the stipulated deadline.</td>\n",
       "      <td>receive well stipulate deadline</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.588, 'pos': 0.412, 'comp...</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f7c4243c7fe1938f181bec41a392bdeb</td>\n",
       "      <td>8e6bfb81e283fa7e4f11123a3fb894f1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Parab�����������������������������������������...</td>\n",
       "      <td>01/03/2018 00:00</td>\n",
       "      <td>02/03/2018 10:26</td>\n",
       "      <td>Congratulations lannister shops I loved to buy...</td>\n",
       "      <td>congratulation lannister shop love buy interne...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.353, 'pos': 0.647, 'comp...</td>\n",
       "      <td>0.9001</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          review_id                          order_id  \\\n",
       "0  e64fb393e7b32834bb789ff8bb30750e  658677c97b385a9be170737859d3511b   \n",
       "1  f7c4243c7fe1938f181bec41a392bdeb  8e6bfb81e283fa7e4f11123a3fb894f1   \n",
       "\n",
       "   review_score review_comment_title  \\\n",
       "0             5                  NaN   \n",
       "1             5                  NaN   \n",
       "\n",
       "                              review_comment_message review_creation_date  \\\n",
       "0              Recebi bem antes do prazo estipulado.     21/04/2017 00:00   \n",
       "1  Parab�����������������������������������������...     01/03/2018 00:00   \n",
       "\n",
       "  review_answer_timestamp                                    english_comment  \\\n",
       "0        21/04/2017 22:02   I received well before the stipulated deadline.    \n",
       "1        02/03/2018 10:26  Congratulations lannister shops I loved to buy...   \n",
       "\n",
       "                                       clean_comment  \\\n",
       "0                    receive well stipulate deadline   \n",
       "1  congratulation lannister shop love buy interne...   \n",
       "\n",
       "                                              scores  compound comp_score  \n",
       "0  {'neg': 0.0, 'neu': 0.588, 'pos': 0.412, 'comp...    0.2732        pos  \n",
       "1  {'neg': 0.0, 'neu': 0.353, 'pos': 0.647, 'comp...    0.9001        pos  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "data_reviews['scores'] = data_reviews['clean_comment'].apply(lambda review: sid.polarity_scores(review))\n",
    "\n",
    "data_reviews['compound']  = data_reviews['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "\n",
    "data_reviews['comp_score'] = data_reviews['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')\n",
    "\n",
    "data_reviews.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "informative-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_order = pd.read_csv(\"./Dataset/olist_order_items_dataset.csv\")\n",
    "data_cat_name = pd.read_csv(\"./Dataset/olist_products_dataset.csv\")\n",
    "data_cat_name_english = pd.read_csv(\"./Dataset/product_category_name_translation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "advisory-coordinate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>shipping_limit_date</th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_comment_title</th>\n",
       "      <th>...</th>\n",
       "      <th>comp_score</th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>product_name_lenght</th>\n",
       "      <th>product_description_lenght</th>\n",
       "      <th>product_photos_qty</th>\n",
       "      <th>product_weight_g</th>\n",
       "      <th>product_length_cm</th>\n",
       "      <th>product_height_cm</th>\n",
       "      <th>product_width_cm</th>\n",
       "      <th>product_category_name_english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00010242fe8c5a6d1ba2dd792cb16214</td>\n",
       "      <td>1</td>\n",
       "      <td>4244733e06e7ecb4970a6e2683c13e61</td>\n",
       "      <td>48436dade18ac8b2bce089ec2a041202</td>\n",
       "      <td>2017-09-19 09:45:35</td>\n",
       "      <td>58.9</td>\n",
       "      <td>13.29</td>\n",
       "      <td>97ca439bc427b48bc1cd7177abe71365</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>pos</td>\n",
       "      <td>cool_stuff</td>\n",
       "      <td>58.0</td>\n",
       "      <td>598.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>cool_stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130898c0987d1801452a8ed92a670612</td>\n",
       "      <td>1</td>\n",
       "      <td>4244733e06e7ecb4970a6e2683c13e61</td>\n",
       "      <td>48436dade18ac8b2bce089ec2a041202</td>\n",
       "      <td>2017-07-05 02:44:11</td>\n",
       "      <td>55.9</td>\n",
       "      <td>17.96</td>\n",
       "      <td>b11cba360bbe71410c291b764753d37f</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>pos</td>\n",
       "      <td>cool_stuff</td>\n",
       "      <td>58.0</td>\n",
       "      <td>598.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>cool_stuff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id  order_item_id  \\\n",
       "0  00010242fe8c5a6d1ba2dd792cb16214              1   \n",
       "1  130898c0987d1801452a8ed92a670612              1   \n",
       "\n",
       "                         product_id                         seller_id  \\\n",
       "0  4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   \n",
       "1  4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   \n",
       "\n",
       "   shipping_limit_date  price  freight_value  \\\n",
       "0  2017-09-19 09:45:35   58.9          13.29   \n",
       "1  2017-07-05 02:44:11   55.9          17.96   \n",
       "\n",
       "                          review_id  review_score review_comment_title  ...  \\\n",
       "0  97ca439bc427b48bc1cd7177abe71365             5                  NaN  ...   \n",
       "1  b11cba360bbe71410c291b764753d37f             5                  NaN  ...   \n",
       "\n",
       "  comp_score product_category_name product_name_lenght  \\\n",
       "0        pos            cool_stuff                58.0   \n",
       "1        pos            cool_stuff                58.0   \n",
       "\n",
       "  product_description_lenght product_photos_qty product_weight_g  \\\n",
       "0                      598.0                4.0            650.0   \n",
       "1                      598.0                4.0            650.0   \n",
       "\n",
       "   product_length_cm product_height_cm product_width_cm  \\\n",
       "0               28.0               9.0             14.0   \n",
       "1               28.0               9.0             14.0   \n",
       "\n",
       "   product_category_name_english  \n",
       "0                     cool_stuff  \n",
       "1                     cool_stuff  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_orderid = pd.merge(data_order, data_reviews, on='order_id')\n",
    "data_order_prodid = pd.merge(data_orderid, data_cat_name, on='product_id')\n",
    "data_prodcat_review = pd.merge(data_order_prodid, data_cat_name_english, on='product_category_name')\n",
    "data_prodcat_review.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caring-secondary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4453"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_reviews = data_prodcat_review[data_prodcat_review['comp_score'] == 'neg']\n",
    "len(neg_reviews.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "centered-honey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_category_name_english\n",
       "health_beauty            9.412407\n",
       "watches_gifts            8.986177\n",
       "bed_bath_table           7.756310\n",
       "sports_leisure           7.396061\n",
       "computers_accessories    6.793716\n",
       "furniture_decor          5.457622\n",
       "housewares               4.725536\n",
       "cool_stuff               4.711356\n",
       "auto                     4.432905\n",
       "garden_tools             3.617572\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sales = data['price'].sum()\n",
    "costliest = data.groupby('product_category_name_english')['price'].sum().sort_values(ascending = False).head(10)\n",
    "(costliest/total_sales)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "recent-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-canvas",
   "metadata": {},
   "source": [
    "## Category bed_bath_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "knowing-highland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 10 WORDS FOR TOPIC #0\n",
      "['pay', 'want', 'wait', 'store', 'order', 'buy', 'curtain', 'product', 'miss', 'receive']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #1\n",
      "['wrong', 'ask', 'color', 'size', 'bad', 'baratheon', 'material', 'low', 'quality', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #2\n",
      "['success', 'problem', 'say', 'request', 'deadline', 'mail', 'time', 'deliver', 'delivery', 'delay']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #3\n",
      "['send', 'buy', 'disappointed', 'come', 'product', 'problem', 'different', 'miss', 'time', 'purchase']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #4\n",
      "['lack', 'cloth', 'glass', 'receive', 'come', 'unit', 'buy', 'want', 'product', 'pay']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #5\n",
      "['problem', 'like', 'delivery', 'office', 'receive', 'post', 'product', 'freight', 'pay', 'buy']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #6\n",
      "['fabric', 'bed', 'len', 'quilt', 'come', 'receive', 'game', 'buy', 'product', 'miss']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #7\n",
      "['miss', 'come', 'disappointed', 'separate', 'gray', 'send', 'merchandise', 'color', 'curtain', 'deliver']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #8\n",
      "['delivery', 'wait', 'little', 'dirty', 'date', 'bad', 'come', 'receive', 'wrong', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #9\n",
      "['queen', 'carpet', 'rug', 'buy', 'product', 'deliver', 'box', 'skirt', 'receive', 'miss']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtm = cv.fit_transform(neg_reviews[neg_reviews['product_category_name_english'] == 'bed_bath_table']['clean_comment'])\n",
    "LDA = LatentDirichletAllocation(n_components=10,random_state=42)\n",
    "LDA.fit(dtm)\n",
    "for index,topic in enumerate(LDA.components_):\n",
    "    print(f'THE TOP 10 WORDS FOR TOPIC #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "falling-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = 'bed_bath_table'\n",
    "negative_words = ['product','deliver', 'wait', 'buy', 'miss', 'wrong', 'bad', 'low', 'problem', 'request', 'deadline', 'time', 'deliver',\n",
    "        'delay', 'dissapointed', 'send', 'purchase', 'lack', 'want', 'separate']\n",
    "def issues(cat, *w):    \n",
    "    d = {}\n",
    "    neg_cat = neg_reviews[neg_reviews['product_category_name_english'] == cat]\n",
    "    for i in range(len(w)):\n",
    "        res = neg_cat[(neg_cat['clean_comment'].str.match(w[i])==True)]\n",
    "        t = int(res['clean_comment'].count())\n",
    "        #print(w[i],t)\n",
    "        for j in range(i+1,len(w)):\n",
    "            r = neg_cat[(neg_cat['clean_comment'].str.match(w[i])==True) & (neg_cat['clean_comment'].str.match(w[j])==True)]\n",
    "            c = int(r['clean_comment'].count())\n",
    "            #print(w[j],c)\n",
    "            t -= c\n",
    "        d.update({w[i] : t})        \n",
    "    return d\n",
    "\n",
    "def filter_words(dic):\n",
    "    sorted_dic = {k: v for k, v in sorted(dic.items(), key=lambda item: item[1], reverse=True)}\n",
    "    lst = [k for k,v in sorted_dic.items() if v > 10]\n",
    "    return lst\n",
    "\n",
    "def output(cat, *words):\n",
    "    for word in words:\n",
    "        a = neg_reviews[neg_reviews['product_category_name_english'] == cat]\n",
    "        b = a[a['clean_comment'].str.match(word)==True]\n",
    "        b.to_csv('./Result/' + cat + '_' + word + '.csv')\n",
    "\n",
    "dic = issues(cat, *negative_words) \n",
    "words = filter_words(dic)\n",
    "output(cat,*words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-pacific",
   "metadata": {},
   "source": [
    "## Category furniture_decor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "automatic-tucson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 10 WORDS FOR TOPIC #0\n",
      "['date', 'defective', 'address', 'answer', 'wait', 'correct', 'come', 'upset', 'deliver', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #1\n",
      "['door', 'complain', 'buy', 'chandelier', 'receive', 'problem', 'come', 'site', 'photo', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #2\n",
      "['ask', 'answer', 'want', 'wrong', 'store', 'buy', 'receive', 'miss', 'come', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #3\n",
      "['unfortunately', 'luminary', 'disappointed', 'complain', 'receive', 'crumple', 'purchase', 'buy', 'delay', 'come']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #4\n",
      "['pay', 'receive', 'open', 'come', 'product', 'arrive', 'complaint', 'buy', 'curtain', 'miss']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #5\n",
      "['come', 'merchandise', 'miss', 'purchase', 'unit', 'freight', 'product', 'receive', 'pay', 'buy']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #6\n",
      "['purchase', 'know', 'touch', 'make', 'pay', 'deliver', 'difficult', 'delay', 'store', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #7\n",
      "['send', 'color', 'invoice', 'possible', 'quality', 'low', 'wrong', 'bad', 'come', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #8\n",
      "['rest', 'cancel', 'item', 'purchase', 'kit', 'receive', 'come', 'product', 'pendant', 'miss']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #9\n",
      "['use', 'try', 'break', 'time', 'solve', 'office', 'arrive', 'post', 'problem', 'delivery']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtm1 = cv.fit_transform(neg_reviews[neg_reviews['product_category_name_english'] == 'furniture_decor']['clean_comment'])\n",
    "LDA1 = LatentDirichletAllocation(n_components=10,random_state=42)\n",
    "LDA1.fit(dtm1)\n",
    "for index,topic in enumerate(LDA1.components_):\n",
    "    print(f'THE TOP 10 WORDS FOR TOPIC #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "super-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = 'furniture_decor'\n",
    "negative_words = ['crumple', 'complain', 'buy', 'defective', 'unfortunately', 'deliver', 'product', 'pay', 'problem', 'manage', 'miss', \n",
    "         'send', 'pendant', 'receive', 'request', 'delivery', 'want', 'wait', 'time', 'arrive', 'break', \n",
    "         'delay', 'complaint', 'cancel', 'disappointed', 'difficult', 'different', 'low', 'bad', 'wrong', 'date', 'arrive']\n",
    "\n",
    "dic = issues(cat, *negative_words)\n",
    "words = filter_words(dic)\n",
    "output(cat,*words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-watershed",
   "metadata": {},
   "source": [
    "## Category computers_accessories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "peaceful-government",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 10 WORDS FOR TOPIC #0\n",
      "['delivery', 'miss', 'lannister', 'cancel', 'paint', 'deliver', 'send', 'damage', 'come', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #1\n",
      "['arrive', 'xl', 'receive', 'leave', 'cartridge', 'mail', 'hp', 'buy', 'disappointed', 'printer']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #2\n",
      "['despite', 'cable', 'correct', 'recommend', 'supplier', 'defect', 'come', 'cancel', 'product', 'purchase']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #3\n",
      "['post', 'want', 'complain', 'receive', 'delivery', 'pay', 'toner', 'day', 'product', 'problem']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #4\n",
      "['fake', 'buy', 'falsify', 'delivery', 'request', 'deliver', 'cartridge', 'miss', 'come', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #5\n",
      "['different', 'email', 'store', 'arrive', 'wait', 'problem', 'send', 'product', 'delivery', 'delay']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #6\n",
      "['black', 'buy', 'product', 'request', 'deliver', 'color', 'cartridge', 'wrong', 'xl', 'come']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #7\n",
      "['ask', 'purchase', 'wrong', 'original', 'pay', 'unfortunately', 'cartridge', 'buy', 'product', 'receive']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #8\n",
      "['huge', 'occasional', 'waste', 'remove', 'office', 'post', 'time', 'freight', 'pay', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #9\n",
      "['money', 'kit', 'low', 'receive', 'time', 'deliver', 'defective', 'return', 'bad', 'product']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtm2 = cv.fit_transform(neg_reviews[neg_reviews['product_category_name_english'] == 'computers_accessories']['clean_comment'])\n",
    "LDA2 = LatentDirichletAllocation(n_components=10,random_state=42)\n",
    "LDA2.fit(dtm2)\n",
    "for index,topic in enumerate(LDA2.components_):\n",
    "    print(f'THE TOP 10 WORDS FOR TOPIC #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aboriginal-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = 'computers_accessories'\n",
    "words = ['arrive', 'wrong', 'damage', 'deliver', 'product', 'request', 'disappointed', 'miss', 'cancel', 'delivery', 'delay', \n",
    "         'waste', 'remove', 'time', 'post', 'request', 'different', 'low', 'pay', 'faulty', 'want', 'defect', 'defective', 'bad', \n",
    "         'ill']\n",
    "dic = issues(cat, *negative_words)\n",
    "words = filter_words(dic)\n",
    "output(cat,*words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-black",
   "metadata": {},
   "source": [
    "## Category health_beauty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "adapted-jefferson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 10 WORDS FOR TOPIC #0\n",
      "['kit', 'expensive', 'miss', 'make', 'purchase', 'product', 'pay', 'request', 'disappointed', 'receive']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #1\n",
      "['brush', 'want', 'company', 'unfortunately', 'invoice', 'send', 'arrive', 'use', 'box', 'bad']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #2\n",
      "['arrive', 'little', 'ml', 'receive', 'miss', 'color', 'kit', 'unit', 'product', 'buy']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #3\n",
      "['end', 'receive', 'device', 'work', 'difficult', 'buy', 'unfortunately', 'product', 'miss', 'scissor']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #4\n",
      "['loose', 'vicete', 'lack', 'miss', 'scissor', 'come', 'complain', 'quality', 'deliver', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #5\n",
      "['color', 'buy', 'deliver', 'send', 'miss', 'site', 'receive', 'wrong', 'come', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #6\n",
      "['package', 'deliver', 'freight', 'store', 'lannister', 'miss', 'receive', 'pay', 'buy', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #7\n",
      "['arrive', 'pay', 'purchase', 'abusive', 'value', 'try', 'long', 'cancel', 'time', 'freight']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #8\n",
      "['purchase', 'office', 'post', 'ill', 'time', 'day', 'buy', 'problem', 'delay', 'delivery']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #9\n",
      "['buy', 'lense', 'come', 'expect', 'place', 'know', 'need', 'receive', 'defective', 'product']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtm3 = cv.fit_transform(neg_reviews[neg_reviews['product_category_name_english'] == 'health_beauty']['clean_comment'])\n",
    "LDA3 = LatentDirichletAllocation(n_components=10,random_state=42)\n",
    "LDA3.fit(dtm3)\n",
    "for index,topic in enumerate(LDA3.components_):\n",
    "    print(f'THE TOP 10 WORDS FOR TOPIC #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "distinguished-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = 'health_beauty'\n",
    "words = ['wait', 'deliver', 'receive', 'wrong', 'product', 'leak', 'arrrive', 'bad', 'delivery', 'delay', 'prica', 'miss', 'defect', \n",
    "         'deadline', 'time', 'defective', 'different', 'abusive', 'try', 'long', 'cancel', 'unfortunately']\n",
    "dic = issues(cat, *negative_words)\n",
    "words = filter_words(dic)\n",
    "output(cat,*words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-boston",
   "metadata": {},
   "source": [
    "## Category sports_leisure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "atmospheric-allah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 10 WORDS FOR TOPIC #0\n",
      "['jump', 'think', 'buy', 'dissatisfied', 'kit', 'freight', 'receive', 'product', 'miss', 'pay']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #1\n",
      "['inside', 'unfortunately', 'come', 'box', 'tube', 'dot', 'polka', 'miss', 'product', 'lack']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #2\n",
      "['expectation', 'stark', 'make', 'delay', 'commitment', 'targaryen', 'ill', 'horrible', 'lot', 'seller']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #3\n",
      "['miss', 'need', 'error', 'send', 'product', 'want', 'buy', 'request', 'disappointed', 'cancel']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #4\n",
      "['fact', 'deliver', 'buy', 'know', 'delivery', 'cancel', 'make', 'miss', 'say', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #5\n",
      "['request', 'solve', 'purchase', 'buy', 'envelope', 'penalty', 'deadline', 'deliver', 'product', 'problem']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #6\n",
      "['buy', 'come', 'complain', 'day', 'office', 'post', 'arrive', 'product', 'delay', 'delivery']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #7\n",
      "['slam', 'kg', 'medicine', 'request', 'buy', 'send', 'product', 'come', 'wrong', 'deliver']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #8\n",
      "['request', 'want', 'know', 'buy', 'contact', 'store', 'come', 'hemo', 'rage', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #9\n",
      "['miss', 'protein', 'order', 'deliver', 'purchase', 'come', 'wrong', 'buy', 'receive', 'product']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtm4 = cv.fit_transform(neg_reviews[neg_reviews['product_category_name_english'] == 'sports_leisure']['clean_comment'])\n",
    "LDA4 = LatentDirichletAllocation(n_components=10,random_state=42)\n",
    "LDA4.fit(dtm4)\n",
    "for index,topic in enumerate(LDA4.components_):\n",
    "    print(f'THE TOP 10 WORDS FOR TOPIC #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "radical-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = 'sports_leisure'\n",
    "words = ['dissatisfied', 'miss', 'product', 'complain', 'delay', 'expect', 'deliver', 'confess', 'disappoint', 'error', \n",
    "         'disappointed', 'cancel', 'penalty', 'delivery', 'wrong', 'post', 'defect', 'problem', 'low', 'lack', 'rage']\n",
    "dic = issues(cat, *negative_words)\n",
    "words = filter_words(dic)\n",
    "output(cat,*words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-territory",
   "metadata": {},
   "source": [
    "## Category watches_gifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "portuguese-weekly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 10 WORDS FOR TOPIC #0\n",
      "['request', 'office', 'post', 'stark', 'dissatisfied', 'wait', 'deadline', 'disappointed', 'product', 'delivery']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #1\n",
      "['buy', 'post', 'office', 'watch', 'send', 'miss', 'pay', 'defect', 'come', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #2\n",
      "['recommend', 'term', 'negative', 'receive', 'buy', 'merchandise', 'unfortunately', 'delivery', 'problem', 'delay']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #3\n",
      "['bracelet', 'cancellation', 'ask', 'speak', 'email', 'mistake', 'lack', 'delivery', 'send', 'pay']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #4\n",
      "['miss', 'appear', 'little', 'request', 'email', 'clock', 'watch', 'buy', 'deliver', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #5\n",
      "['wrong', 'letter', 'come', 'deliver', 'request', 'defective', 'warn', 'mail', 'receive', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #6\n",
      "['post', 'office', 'come', 'ill', 'unit', 'happen', 'need', 'know', 'miss', 'buy']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #7\n",
      "['request', 'pay', 'ask', 'return', 'buy', 'arrive', 'day', 'purchase', 'cancel', 'product']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #8\n",
      "['pack', 'contact', 'dissatisfied', 'model', 'lannister', 'come', 'make', 'deliver', 'purchase', 'wrong']\n",
      "\n",
      "\n",
      "THE TOP 10 WORDS FOR TOPIC #9\n",
      "['receive', 'know', 'quality', 'low', 'store', 'want', 'product', 'freight', 'pay', 'bad']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtm5 = cv.fit_transform(neg_reviews[neg_reviews['product_category_name_english'] == 'watches_gifts']['clean_comment'])\n",
    "LDA5 = LatentDirichletAllocation(n_components=10,random_state=42)\n",
    "LDA5.fit(dtm5)\n",
    "for index,topic in enumerate(LDA5.components_):\n",
    "    print(f'THE TOP 10 WORDS FOR TOPIC #{index}')\n",
    "    print([cv.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "behavioral-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = 'watches_gifts'\n",
    "words = ['deadline', 'wait', 'disappointed', 'post', 'miss', 'cancel', 'delivery', 'product', 'low', 'change', 'wrong', \n",
    "         'defective', 'unfortunately', 'deliver', 'problem', 'warn', 'request', 'delay', 'exchange', 'defect', 'time', 'return',\n",
    "         'bad', 'badly', 'dissatisfied']\n",
    "dic = issues(cat, *negative_words)\n",
    "words = filter_words(dic)\n",
    "output(cat,*words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-buffer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
